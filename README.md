“I implemented an Augmented RAG system in Node.js. During indexing, I ingest local documents, chunk them with overlap, embed each chunk using OpenAI embeddings, and store vectors + metadata in a local JSON vector store. At query time, I augment the user question with multi-query rewrites and a HyDE-generated pseudo-answer to improve recall. I embed those variants, retrieve top chunks via cosine similarity, diversify context using an MMR-like approach, and then call the LLM with strict ‘context-only’ instructions to reduce hallucinations and return answer + citations. The backend is exposed through a Fastify API so the frontend never touches OpenAI keys. I also added caching and retry with exponential backoff for rate limits/transient failures.”
